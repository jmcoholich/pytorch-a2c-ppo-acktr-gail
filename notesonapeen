do some sanity checks again -- can I learn to do something really simple like avoid termination based on one condition only? Then progressively scale up complexity.

Still keep termination conditions for falling and height. 


Ok so keep the same state space representation for all. Things to decide:

-reward
-termination conditions
-hyperparams (including number of frames to skip and timeout)

Simplest case: 
- learn to avoid termination. Get reward of +1 every time.

Reward: +1.0 at every timestep
Termination: Falling, timeout

defaul kostrikov command:

python main.py --env-name "Reacher-v2" --algo ppo --use-gae --log-interval 1 --num-steps 2048 --num-processes 1 --lr 3e-4 --entropy-coef 0 --value-loss-coef 0.5 --ppo-epoch 10 --num-mini-batch 32 --gamma 0.99 --gae-lambda 0.95 --num-env-steps 1000000 --use-linear-lr-decay --use-proper-time-limits



python main.py --algo=ppo --clip-param=0.2 --entropy-coef=0 --env-name=gym_aliengo:aliengo-v0 --gae-lambda=0.95 --gamma=0.99 --log-interval=1 --lr=3e-4 --max-grad-norm=0.5 --num-env-steps=10000000 --num-mini-batch=32 --num-processes=10 --num-steps=100 --ppo-epoch=10 --use-linear-lr-decay=False --value-loss-coef=0.5 --use-gae --use-proper-time-limits --wandb-project text_reward_calculation --seed=1

plot entropy, automatically save gym videos

do a test where the reward is always 1 and it always times out after 100 steps, see what episode reward is.


So it seems that the episode reward is just the reward of every step summed. 

So once this sweep finishes, test the top N runs across multiple random seeds.

Ok so now I see what is successful on this task. Default hyperparms for this task will be:

num_steps * num_processes * .08 =  timeout episode length 
lr = 5e-5 
ppo_epoch = 60
num_mini_batch = 10 

Now, run several runs to verify this:
python main.py --algo=ppo --clip-param=0.2 --entropy-coef=0 --env-name=gym_aliengo:aliengo-v0 --gae-lambda=0.95 --gamma=0.99 --log-interval=1 --lr=5e-5 --max-grad-norm=0.5 --num-env-steps=1000000 --num-mini-batch=10 --num-processes=5 --num-steps=160 --ppo-epoch=60 --use-linear-lr-decay=False --value-loss-coef=0.5 --use-gae --use-proper-time-limits --wandb-project Experiment1_more_logging --seed=1

Once these four turn out well, do another hyperparam sweep over gamma, gae_lambda, value_loss_coef, max_grad_norm 

du -hsc * | sort -rh

After I find the best hyperparms for this one, then add x-velocity to reward.

Ok the main thing I learned from the second sweep was, keep the max grad norm at 0.5. Hyper params I will keep are: 

num_steps * num_processes * .08 =  timeout episode length 
lr = 5e-5 
ppo_epoch = 60
num_mini_batch = 10 
max_grad_norm = 0.5
value_loss_coef = 0.5
gamma = 0.99
GAE_lambda = 0.95
clip_param = 0.2

Added x_vel and torque pen to reward, only change in env. Run:

python main.py --algo=ppo --clip-param=0.2 --entropy-coef=0.001 --env-name=gym_aliengo:aliengo-v0 --gae-lambda=0.95 --gamma=0.99 --log-interval=1 --lr=5e-5 --max-grad-norm=0.5 --num-env-steps=2000000 --num-mini-batch=10 --num-processes=10 --num-steps=80 --ppo-epoch=60 --use-linear-lr-decay=False --value-loss-coef=0.5 --use-gae --use-proper-time-limits --wandb-project Experiment2 --seed=10


Also train the one that Andrew saw for longer. *** Do this once experiment 3 runs are over ***

If experiment 3 learns anything, do the same thing but increase the timeout and take more samples accordingly. 


If experiment 3 doesn't work, try an existence rew of 0.1, and maybe a longer timeout. *** Do this once experiment 3 runs are over ***

Also, once these runs finish, change the env so that I can run multiple envs at once.

Try increasing gamma since this task involves longer time horizons: Running:
python main.py --algo=ppo --clip-param=0.2 --entropy-coef=0.0101 --env-name=gym_aliengo:aliengo-v0 --gae-lambda=0.95 --gamma=0.999 --log-interval=1 --lr=5e-5 --max-grad-norm=0.5 --num-env-steps=3_000_000 --num-mini-batch=10 --num-processes=10 --num-steps=80 --ppo-epoch=60 --use-linear-lr-decay=False --value-loss-coef=0.5 --use-gae --use-proper-time-limits --wandb-project Experiment3 --seed=60


So now I'm at a loss for what to do 
-- run a new hyperparam search with this env, increasing cap to 1000. 

So actually a new hyperparam search is probably in order now that I augmented the observation space AND fixed the previous issue with multiple environments. All work before that is essentially void. Also eps len limit is probably a go? Do a sweep similar to my first sweep. Probably fix the frames for like 2 seconds. 


yeah... so I didn't actually read papers...

I should just fix my environment to what I actually want to learn, then keep it at that and optimize hyperparms.

I need a better note taking system for keeping track of things that I've done. 

How is what I'm doing now something that I've never done before? 
- I'm doing it after augmenting the observation space
- I haven't added a bunch of termination conditions or skipped frames: its just the original environment. 

For meeting update:
tried no termination conditions, with and without making them penalties
skipped frames
nothing worked, did sanity checks where I verified that nothing was wrong and I could learn simple things
Just did hyperparm sweep with updated env where I have the augmented observation, fixed various other issues. 

Wandb questions: Can  i run a random hyper param sweep, but every sample of hyper params is run on 4 different random seeds?

Best hyper params so far, based off of viewing grouped runs.
lr: 5e-5
num_steps (with 10 processes): 320 (best) or 800 (vs 80(worst) and 160)
num_mini_batch: seems like 32 is best, not a super strong signal though
ppo_epoch: very clearly, 60 
entropy: too noisy, no clear winner

Registered for courses.


Things to run: 3 more random seeds of elated-sweep-8
4 random seeds of elated-sweep- with the best guessed params

looks like only 5 and 6 and 1 are being used? it seems like 2 and 4 are the best.

Then send zsolt update.
fine grained hyperparm search around elated-sweep
elated-sweep on a computer where I try 100 processes to speed it up.


elated-sweep:
python main.py --algo=ppo --clip-param=0.2 --entropy-coef=1e-06 --env-name=gym_aliengo:aliengo-v0 --gae-lambda=0.95 --gamma=0.99 --log-interval=1 --lr=5e-05 --max-grad-norm=0.5 --num-env-steps=100000000 --num-mini-batch=16 --num-processes=10 --num-steps=800 --ppo-epoch=60 --seed=912627 --use-linear-lr-decay=False --value-loss-coef=0.5 --wandb-project Experiment4

I need to be able to resume training from a saved policy.

Best guessed params:
python main.py --algo=ppo --clip-param=0.2 --entropy-coef=0.0 --env-name=gym_aliengo:aliengo-v0 --gae-lambda=0.95 --gamma=0.99 --log-interval=1 --lr=5e-05 --max-grad-norm=0.5 --num-env-steps=100000000 --num-mini-batch=32 --num-processes=10 --num-steps=320 --ppo-epoch=60 --seed=1000000 --use-linear-lr-decay=False --value-loss-coef=0.5 --wandb-project Experiment4


Sweep:
wandb agent jcoholich/Experiment4/hwuj8siv


What am I hung up on? 
- whether or not I should take advantage of all remaining idle computing power for my hyperparm sweep and whether I should make a post in the slack about it. I think I'm good for now. Also, I need to just send zsolt an update. and save come compute for my project. I think its good. These runs may last for a long time so its good not to hog too much compute.


Maybe try everything same as elated run but with lower lr. Increase save frequency.

A faster decrease in entropy is correlated with an increase in average episode duration, but not of reward neccesarily.

Investigate how the reward is what it is when the videos show zero displacement. Is there something wrong with my reward? 
Get the save video to print reward earned.

Also add back the save frames -- I think it will make things more predictable.

Todo right now
- start project


hypotheses:
learning rate inversely correclated with num of param
not having hold steps makes the reward signal super noisy, since at the next step the nn can give another command that 
obliterates the input from the previouis command.

I should define my total search space in terms of hyperparameters and environment characteristics.

Also anything I did with linear lr decay on is also void.

Consider adding terminations back in ... armed with my better hyperparameters?
I need to switch notes apps, read and talk more about doing productive actual phd research, need a better way to keep track of my own progress.


Next steps for Zsolt
- Add frame skips back in
- Define my whole search space, reorganize my thoughts and reread notes and papers to ensure I don't duplicate work/ sweeps
- 


